{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4629ac4a-fbea-446c-acff-ab88d896a259",
   "metadata": {},
   "source": [
    "# Soft Decision Tree distilled from Neural Network\n",
    "\n",
    "Based on \"Distilling a Neural Network Into a Soft Decision Tree\" by Nicholas Frosst, Geoffrey Hinton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2210eb37-bfe3-4c30-a32e-a3ff23baad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.signal as signal\n",
    "import scipy.stats as stats\n",
    "import uncertainty_toolbox as uct\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Any, Dict, List, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_auc_score\n",
    "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from src.probabilistic_flow_boosting.extras.datasets.uci_dataset import UCIDataSet\n",
    "from src.probabilistic_flow_boosting.tfboost.tree import EmbeddableCatBoostPriorNormal\n",
    "from src.probabilistic_flow_boosting.tfboost.tfboost import TreeFlowBoost\n",
    "from src.probabilistic_flow_boosting.tfboost.flow import ContinuousNormalizingFlow\n",
    "from src.probabilistic_flow_boosting.pipelines.reporting.nodes import calculate_nll\n",
    "from src.probabilistic_flow_boosting.pipelines.modeling.utils import setup_random_seed\n",
    "\n",
    "from src.probabilistic_flow_boosting.tfboost.soft_decision_tree import SoftDecisionTree\n",
    "\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89702633-2ac7-4f86-b30a-8f051f3a5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "TRAIN = False\n",
    "MODEL_FILEPATH = 'treeflow_wine.model'\n",
    "\n",
    "setup_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef43411e-4bce-4ba4-b5ec-c3e4476f338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = UCIDataSet(\n",
    "    filepath_data = \"data/01_raw/UCI/wine-quality-red/data.txt\",\n",
    "    filepath_index_columns = \"data/01_raw/UCI/wine-quality-red/index_features.txt\",\n",
    "    filepath_index_rows = \"data/01_raw/UCI/wine-quality-red/index_train_1.txt\"\n",
    ").load()\n",
    "y_train = UCIDataSet(\n",
    "    filepath_data = \"data/01_raw/UCI/wine-quality-red/data.txt\",\n",
    "    filepath_index_columns = \"data/01_raw/UCI/wine-quality-red/index_target.txt\",\n",
    "    filepath_index_rows = \"data/01_raw/UCI/wine-quality-red/index_train_1.txt\"\n",
    ").load()\n",
    "\n",
    "x_test = UCIDataSet(\n",
    "    filepath_data = \"data/01_raw/UCI/wine-quality-red/data.txt\",\n",
    "    filepath_index_columns = \"data/01_raw/UCI/wine-quality-red/index_features.txt\",\n",
    "    filepath_index_rows = \"data/01_raw/UCI/wine-quality-red/index_test_1.txt\"\n",
    ").load()\n",
    "y_test = UCIDataSet(\n",
    "    filepath_data = \"data/01_raw/UCI/wine-quality-red/data.txt\",\n",
    "    filepath_index_columns = \"data/01_raw/UCI/wine-quality-red/index_target.txt\",\n",
    "    filepath_index_rows = \"data/01_raw/UCI/wine-quality-red/index_test_1.txt\"\n",
    ").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f02ce557-174e-49b8-b581-5b781d4bd30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4703c7-7a01-4db0-b804-6e3351b5f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = torch.Tensor(x_tr.values)\n",
    "x_val = torch.Tensor(x_val.values)\n",
    "x_test = torch.Tensor(x_test.values)\n",
    "\n",
    "y_tr = torch.Tensor(y_tr.values)\n",
    "y_val = torch.Tensor(y_val.values)\n",
    "y_test = torch.Tensor(y_test.values)\n",
    "# , x_val, y_tr, y_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da2acc4d-f259-4855-8403-493e5a97a5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/datascience/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Test Loss: 0.562272155204594\n",
      "--------------------------------------------------\n",
      "Epoch: 1 | Test Loss: 0.5611798371296229\n",
      "--------------------------------------------------\n",
      "Epoch: 2 | Test Loss: 0.5601760044986146\n",
      "--------------------------------------------------\n",
      "Epoch: 3 | Test Loss: 0.5592155641342641\n",
      "--------------------------------------------------\n",
      "Epoch: 4 | Test Loss: 0.5582763908538477\n",
      "--------------------------------------------------\n",
      "Epoch: 5 | Test Loss: 0.5573468284996032\n",
      "--------------------------------------------------\n",
      "Epoch: 6 | Test Loss: 0.5564243781497276\n",
      "--------------------------------------------------\n",
      "Epoch: 7 | Test Loss: 0.555506905438887\n",
      "--------------------------------------------------\n",
      "Epoch: 8 | Test Loss: 0.5545917601486131\n",
      "--------------------------------------------------\n",
      "Epoch: 9 | Test Loss: 0.5536797193442786\n",
      "--------------------------------------------------\n",
      "Epoch: 10 | Test Loss: 0.5527701274570335\n",
      "--------------------------------------------------\n",
      "Epoch: 11 | Test Loss: 0.5518612684934698\n",
      "--------------------------------------------------\n",
      "Epoch: 12 | Test Loss: 0.5509532903268192\n",
      "--------------------------------------------------\n",
      "Epoch: 13 | Test Loss: 0.5500476421567854\n",
      "--------------------------------------------------\n",
      "Epoch: 14 | Test Loss: 0.549142622973681\n",
      "--------------------------------------------------\n",
      "Epoch: 15 | Test Loss: 0.548240506934756\n",
      "--------------------------------------------------\n",
      "Epoch: 16 | Test Loss: 0.5473381866289465\n",
      "--------------------------------------------------\n",
      "Epoch: 17 | Test Loss: 0.5464376729418355\n",
      "--------------------------------------------------\n",
      "Epoch: 18 | Test Loss: 0.5455380521937313\n",
      "--------------------------------------------------\n",
      "Epoch: 19 | Test Loss: 0.544639401768591\n",
      "--------------------------------------------------\n",
      "Epoch: 20 | Test Loss: 0.5437412636397317\n",
      "--------------------------------------------------\n",
      "Epoch: 21 | Test Loss: 0.5428451043581172\n",
      "--------------------------------------------------\n",
      "Epoch: 22 | Test Loss: 0.5419491251505419\n",
      "--------------------------------------------------\n",
      "Epoch: 23 | Test Loss: 0.541053180026142\n",
      "--------------------------------------------------\n",
      "Epoch: 24 | Test Loss: 0.5401592063510255\n",
      "--------------------------------------------------\n",
      "Epoch: 25 | Test Loss: 0.5392671893677489\n",
      "--------------------------------------------------\n",
      "Epoch: 26 | Test Loss: 0.53837462933132\n",
      "--------------------------------------------------\n",
      "Epoch: 27 | Test Loss: 0.537483051436977\n",
      "--------------------------------------------------\n",
      "Epoch: 28 | Test Loss: 0.5365927814778051\n",
      "--------------------------------------------------\n",
      "Epoch: 29 | Test Loss: 0.5357023671756971\n",
      "--------------------------------------------------\n",
      "Epoch: 30 | Test Loss: 0.5348137396019573\n",
      "--------------------------------------------------\n",
      "Epoch: 31 | Test Loss: 0.5339259153703932\n",
      "--------------------------------------------------\n",
      "Epoch: 32 | Test Loss: 0.5330392463808246\n",
      "--------------------------------------------------\n",
      "Epoch: 33 | Test Loss: 0.5321536886271659\n",
      "--------------------------------------------------\n",
      "Epoch: 34 | Test Loss: 0.5312668292801958\n",
      "--------------------------------------------------\n",
      "Epoch: 35 | Test Loss: 0.5303838063519712\n",
      "--------------------------------------------------\n",
      "Epoch: 36 | Test Loss: 0.5294988105447578\n",
      "--------------------------------------------------\n",
      "Epoch: 37 | Test Loss: 0.5286153900450834\n",
      "--------------------------------------------------\n",
      "Epoch: 38 | Test Loss: 0.5277344312212118\n",
      "--------------------------------------------------\n",
      "Epoch: 39 | Test Loss: 0.5268531306665374\n",
      "--------------------------------------------------\n",
      "Epoch: 40 | Test Loss: 0.5259710081883517\n",
      "--------------------------------------------------\n",
      "Epoch: 41 | Test Loss: 0.5250926253943491\n",
      "--------------------------------------------------\n",
      "Epoch: 42 | Test Loss: 0.5242136298540885\n",
      "--------------------------------------------------\n",
      "Epoch: 43 | Test Loss: 0.5233361951178767\n",
      "--------------------------------------------------\n",
      "Epoch: 44 | Test Loss: 0.5224585036937082\n",
      "--------------------------------------------------\n",
      "Epoch: 45 | Test Loss: 0.5215826874514793\n",
      "--------------------------------------------------\n",
      "Epoch: 46 | Test Loss: 0.5207065936697446\n",
      "--------------------------------------------------\n",
      "Epoch: 47 | Test Loss: 0.5198313420811925\n",
      "--------------------------------------------------\n",
      "Epoch: 48 | Test Loss: 0.5189552268853226\n",
      "--------------------------------------------------\n",
      "Epoch: 49 | Test Loss: 0.5180816696080556\n",
      "--------------------------------------------------\n",
      "Epoch: 50 | Test Loss: 0.5172096844410196\n",
      "--------------------------------------------------\n",
      "Epoch: 51 | Test Loss: 0.5163389715170486\n",
      "--------------------------------------------------\n",
      "Epoch: 52 | Test Loss: 0.5154677128679395\n",
      "--------------------------------------------------\n",
      "Epoch: 53 | Test Loss: 0.5145963432942823\n",
      "--------------------------------------------------\n",
      "Epoch: 54 | Test Loss: 0.5137266154870024\n",
      "--------------------------------------------------\n",
      "Epoch: 55 | Test Loss: 0.5128571173246281\n",
      "--------------------------------------------------\n",
      "Epoch: 56 | Test Loss: 0.5119880828132788\n",
      "--------------------------------------------------\n",
      "Epoch: 57 | Test Loss: 0.5111215874846782\n",
      "--------------------------------------------------\n",
      "Epoch: 58 | Test Loss: 0.5102551003441637\n",
      "--------------------------------------------------\n",
      "Epoch: 59 | Test Loss: 0.509388335403802\n",
      "--------------------------------------------------\n",
      "Epoch: 60 | Test Loss: 0.5085241564098842\n",
      "--------------------------------------------------\n",
      "Epoch: 61 | Test Loss: 0.5076609067350076\n",
      "--------------------------------------------------\n",
      "Epoch: 62 | Test Loss: 0.5067967877695755\n",
      "--------------------------------------------------\n",
      "Epoch: 63 | Test Loss: 0.5059318474201253\n",
      "--------------------------------------------------\n",
      "Epoch: 64 | Test Loss: 0.5050693596104469\n",
      "--------------------------------------------------\n",
      "Epoch: 65 | Test Loss: 0.504207865815555\n",
      "--------------------------------------------------\n",
      "Epoch: 66 | Test Loss: 0.5033467922137573\n",
      "--------------------------------------------------\n",
      "Epoch: 67 | Test Loss: 0.5024864045639913\n",
      "--------------------------------------------------\n",
      "Epoch: 68 | Test Loss: 0.5016279210265601\n",
      "--------------------------------------------------\n",
      "Epoch: 69 | Test Loss: 0.5007690237757721\n",
      "--------------------------------------------------\n",
      "Epoch: 70 | Test Loss: 0.4999109559773442\n",
      "--------------------------------------------------\n",
      "Epoch: 71 | Test Loss: 0.4990534299566232\n",
      "--------------------------------------------------\n",
      "Epoch: 72 | Test Loss: 0.4981977246786253\n",
      "--------------------------------------------------\n",
      "Epoch: 73 | Test Loss: 0.4973423048578934\n",
      "--------------------------------------------------\n",
      "Epoch: 74 | Test Loss: 0.4964861315117488\n",
      "--------------------------------------------------\n",
      "Epoch: 75 | Test Loss: 0.4956313119715945\n",
      "--------------------------------------------------\n",
      "Epoch: 76 | Test Loss: 0.49477731784293083\n",
      "--------------------------------------------------\n",
      "Epoch: 77 | Test Loss: 0.49392399250619917\n",
      "--------------------------------------------------\n",
      "Epoch: 78 | Test Loss: 0.49307289749714434\n",
      "--------------------------------------------------\n",
      "Epoch: 79 | Test Loss: 0.492220976698548\n",
      "--------------------------------------------------\n",
      "Epoch: 80 | Test Loss: 0.4913699240601188\n",
      "--------------------------------------------------\n",
      "Epoch: 81 | Test Loss: 0.4905197170976916\n",
      "--------------------------------------------------\n",
      "Epoch: 82 | Test Loss: 0.48967179385262877\n",
      "--------------------------------------------------\n",
      "Epoch: 83 | Test Loss: 0.4888226165550538\n",
      "--------------------------------------------------\n",
      "Epoch: 84 | Test Loss: 0.4879739701532719\n",
      "--------------------------------------------------\n",
      "Epoch: 85 | Test Loss: 0.48712757045681204\n",
      "--------------------------------------------------\n",
      "Epoch: 86 | Test Loss: 0.48627972479028947\n",
      "--------------------------------------------------\n",
      "Epoch: 87 | Test Loss: 0.48543359074811804\n",
      "--------------------------------------------------\n",
      "Epoch: 88 | Test Loss: 0.48458838462656895\n",
      "--------------------------------------------------\n",
      "Epoch: 89 | Test Loss: 0.4837423588918396\n",
      "--------------------------------------------------\n",
      "Epoch: 90 | Test Loss: 0.4828992121748173\n",
      "--------------------------------------------------\n",
      "Epoch: 91 | Test Loss: 0.4820561569321685\n",
      "--------------------------------------------------\n",
      "Epoch: 92 | Test Loss: 0.4812141845498888\n",
      "--------------------------------------------------\n",
      "Epoch: 93 | Test Loss: 0.4803720599218616\n",
      "--------------------------------------------------\n",
      "Epoch: 94 | Test Loss: 0.47953036230347695\n",
      "--------------------------------------------------\n",
      "Epoch: 95 | Test Loss: 0.47868956434229193\n",
      "--------------------------------------------------\n",
      "Epoch: 96 | Test Loss: 0.4778503360408762\n",
      "--------------------------------------------------\n",
      "Epoch: 97 | Test Loss: 0.4770107974863662\n",
      "--------------------------------------------------\n",
      "Epoch: 98 | Test Loss: 0.4761725604002222\n",
      "--------------------------------------------------\n",
      "Epoch: 99 | Test Loss: 0.4753354923400783\n",
      "--------------------------------------------------\n",
      "Epoch: 100 | Test Loss: 0.47449800835671474\n",
      "--------------------------------------------------\n",
      "Epoch: 101 | Test Loss: 0.47366038588481135\n",
      "--------------------------------------------------\n",
      "Epoch: 102 | Test Loss: 0.47282461315168717\n",
      "--------------------------------------------------\n",
      "Epoch: 103 | Test Loss: 0.47198943714671854\n",
      "--------------------------------------------------\n",
      "Epoch: 104 | Test Loss: 0.4711551983965451\n",
      "--------------------------------------------------\n",
      "Epoch: 105 | Test Loss: 0.4703222820831968\n",
      "--------------------------------------------------\n",
      "Epoch: 106 | Test Loss: 0.46948946800240954\n",
      "--------------------------------------------------\n",
      "Epoch: 107 | Test Loss: 0.46865544248464747\n",
      "--------------------------------------------------\n",
      "Epoch: 108 | Test Loss: 0.46782434691289876\n",
      "--------------------------------------------------\n",
      "Epoch: 109 | Test Loss: 0.466994651136641\n",
      "--------------------------------------------------\n",
      "Epoch: 110 | Test Loss: 0.4661630240170643\n",
      "--------------------------------------------------\n",
      "Epoch: 111 | Test Loss: 0.46533289941860395\n",
      "--------------------------------------------------\n",
      "Epoch: 112 | Test Loss: 0.4645029309183378\n",
      "--------------------------------------------------\n",
      "Epoch: 113 | Test Loss: 0.4636746905044164\n",
      "--------------------------------------------------\n",
      "Epoch: 114 | Test Loss: 0.462847586489578\n",
      "--------------------------------------------------\n",
      "Epoch: 115 | Test Loss: 0.4620204925689516\n",
      "--------------------------------------------------\n",
      "Epoch: 116 | Test Loss: 0.4611949309545536\n",
      "--------------------------------------------------\n",
      "Epoch: 117 | Test Loss: 0.46036993166406276\n",
      "--------------------------------------------------\n",
      "Epoch: 118 | Test Loss: 0.45954437362791817\n",
      "--------------------------------------------------\n",
      "Epoch: 119 | Test Loss: 0.4587189901405744\n",
      "--------------------------------------------------\n",
      "Epoch: 120 | Test Loss: 0.45789418712324503\n",
      "--------------------------------------------------\n",
      "Epoch: 121 | Test Loss: 0.45707083709057555\n",
      "--------------------------------------------------\n",
      "Epoch: 122 | Test Loss: 0.45624926725238735\n",
      "--------------------------------------------------\n",
      "Epoch: 123 | Test Loss: 0.45542884740568307\n",
      "--------------------------------------------------\n",
      "Epoch: 124 | Test Loss: 0.4546082289533921\n",
      "--------------------------------------------------\n",
      "Epoch: 125 | Test Loss: 0.45378561570782405\n",
      "--------------------------------------------------\n",
      "Epoch: 126 | Test Loss: 0.45296557314769265\n",
      "--------------------------------------------------\n",
      "Epoch: 127 | Test Loss: 0.4521464015254812\n",
      "--------------------------------------------------\n",
      "Epoch: 128 | Test Loss: 0.45132870721253954\n",
      "--------------------------------------------------\n",
      "Epoch: 129 | Test Loss: 0.45051079299705116\n",
      "--------------------------------------------------\n",
      "Epoch: 130 | Test Loss: 0.44969189186122366\n",
      "--------------------------------------------------\n",
      "Epoch: 131 | Test Loss: 0.4488758934902306\n",
      "--------------------------------------------------\n",
      "Epoch: 132 | Test Loss: 0.4480593254549329\n",
      "--------------------------------------------------\n",
      "Epoch: 133 | Test Loss: 0.4472458125672362\n",
      "--------------------------------------------------\n",
      "Epoch: 134 | Test Loss: 0.44643013484620525\n",
      "--------------------------------------------------\n",
      "Epoch: 135 | Test Loss: 0.4456165903946677\n",
      "--------------------------------------------------\n",
      "Epoch: 136 | Test Loss: 0.4448038657880715\n",
      "--------------------------------------------------\n",
      "Epoch: 137 | Test Loss: 0.443992025193965\n",
      "--------------------------------------------------\n",
      "Epoch: 138 | Test Loss: 0.44317850315698776\n",
      "--------------------------------------------------\n",
      "Epoch: 139 | Test Loss: 0.44236649423967217\n",
      "--------------------------------------------------\n",
      "Epoch: 140 | Test Loss: 0.44155590179929793\n",
      "--------------------------------------------------\n",
      "Epoch: 141 | Test Loss: 0.4407457118699379\n",
      "--------------------------------------------------\n",
      "Epoch: 142 | Test Loss: 0.43993696539120875\n",
      "--------------------------------------------------\n",
      "Epoch: 143 | Test Loss: 0.4391272422085857\n",
      "--------------------------------------------------\n",
      "Epoch: 144 | Test Loss: 0.4383197249994361\n",
      "--------------------------------------------------\n",
      "Epoch: 145 | Test Loss: 0.43751009672109664\n",
      "--------------------------------------------------\n",
      "Epoch: 146 | Test Loss: 0.4367030014082861\n",
      "--------------------------------------------------\n",
      "Epoch: 147 | Test Loss: 0.4358982252312327\n",
      "--------------------------------------------------\n",
      "Epoch: 148 | Test Loss: 0.4350902556793135\n",
      "--------------------------------------------------\n",
      "Epoch: 149 | Test Loss: 0.43428673034105336\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "input_dim = x_tr.shape[1]  # the number of input dimensions\n",
    "output_dim = y_tr.shape[1] # the number of outputs (i.e., # classes on MNIST)\n",
    "depth = 3              # tree depth\n",
    "lamda = 1e-3           # coefficient of the regularization term\n",
    "lr = 1e-3              # learning rate\n",
    "weight_decaly = 5e-4   # weight decay\n",
    "batch_size = 128       # batch size\n",
    "epochs = 150            # the number of training epochs\n",
    "log_interval = 100     # the number of batches to wait before printing logs\n",
    "use_cuda = False       # whether to use GPU\n",
    "\n",
    "# Model and Optimizer\n",
    "tree = SoftDecisionTree(input_dim, output_dim, depth, lamda, use_cuda)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    tree.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decaly\n",
    ")\n",
    "\n",
    "# Load data\n",
    "train_loader: DataLoader = DataLoader(\n",
    "    dataset=TensorDataset(x_tr, y_tr),\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_loader: DataLoader = DataLoader(\n",
    "    dataset=TensorDataset(x_val, y_val),\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_loader: DataLoader = DataLoader(\n",
    "    dataset=TensorDataset(x_test, y_test),\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Utils\n",
    "best_testing_acc = np.infty\n",
    "testing_acc_list = []\n",
    "training_loss_list = []\n",
    "criterion = nn.MSELoss(reduce = 'sum')\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Training\n",
    "    tree.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output, penalty = tree.forward(data, is_training_data=True)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss += penalty\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluating\n",
    "    tree.eval()\n",
    "    l = 0.\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = tree.forward(data)\n",
    "        l += criterion(output, target)\n",
    "\n",
    "    accuracy = np.sqrt(l.detach().numpy().item() / len(val_loader.dataset))\n",
    "\n",
    "    if accuracy < best_testing_acc:\n",
    "        best_testing_acc = accuracy\n",
    "\n",
    "    msg = f\"Epoch: {epoch} | Test Loss: {accuracy}\"\n",
    "    print(msg)\n",
    "    print('-'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8a8d2e-5744-4564-a71c-18860a0b5626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4626022019745783\n"
     ]
    }
   ],
   "source": [
    "l = 0.\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    output = tree.forward(data)\n",
    "    l += criterion(output, target)\n",
    "\n",
    "accuracy = np.sqrt(l.detach().numpy().item() / len(test_loader.dataset))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e557cc84-29e6-4c74-84e5-7b7bc0dc646f",
   "metadata": {},
   "source": [
    "## Exploration of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cb59e15-a118-4816-991d-0c59b13c6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, _ = tree._forward(x_test) # Probability of landing in a particular node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "712b73ca-0fce-42a0-8646-93f6dcc10afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9975e-01, 2.8127e-05, 8.6655e-05, 1.4065e-05, 1.2591e-05, 2.4486e-05,\n",
       "        6.1152e-05, 2.3365e-05], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d8d05f3-efac-4c31-ac73-434ed7c3db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = x_test.shape[0]\n",
    "\n",
    "X = tree._data_augment(x_test)\n",
    "\n",
    "path_prob = tree.inner_nodes(X)\n",
    "path_prob = torch.unsqueeze(path_prob, dim=2)\n",
    "path_prob = torch.cat((path_prob, 1 - path_prob), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3de337f-5fbe-40dc-9b73-82441ecdf03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 1, 2])\n",
      "tensor([[9.9988e-01, 1.2159e-04]], grad_fn=<SelectBackward0>)\n",
      "torch.Size([160, 2, 2])\n",
      "tensor([[9.9990e-01, 1.0073e-04],\n",
      "        [3.0492e-01, 6.9508e-01]], grad_fn=<SelectBackward0>)\n",
      "torch.Size([160, 4, 2])\n",
      "tensor([[9.9997e-01, 2.8133e-05],\n",
      "        [8.6036e-01, 1.3964e-01],\n",
      "        [3.3958e-01, 6.6042e-01],\n",
      "        [7.2354e-01, 2.7646e-01]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "_mu = X.data.new(batch_size, 1, 1).fill_(1.0)\n",
    "_penalty = torch.tensor(0.0).to(tree.device)\n",
    "\n",
    "# Iterate through internal odes in each layer to compute the final path\n",
    "# probabilities and the regularization term.\n",
    "begin_idx = 0\n",
    "end_idx = 1\n",
    "\n",
    "for layer_idx in range(0, tree.depth):\n",
    "    _path_prob = path_prob[:, begin_idx:end_idx, :]\n",
    "    print(_path_prob.shape)\n",
    "    print(_path_prob[0])\n",
    "\n",
    "    _mu = _mu.view(batch_size, -1, 1).repeat(1, 1, 2)\n",
    "    _mu = _mu * _path_prob  # update path probabilities\n",
    "\n",
    "    begin_idx = end_idx\n",
    "    end_idx = begin_idx + 2 ** (layer_idx + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33af297b-2938-4694-9344-8b3eb55ac68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.leaf_node_num_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce99dce2-7bd5-484c-9094-5d556f6bfec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9975e-01, 2.8127e-05],\n",
       "        [8.6655e-05, 1.4065e-05],\n",
       "        [1.2591e-05, 2.4486e-05],\n",
       "        [6.1152e-05, 2.3365e-05]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability of \n",
    "_mu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc998e5-78ed-420d-8e2a-f03068efdacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = _mu.view(batch_size, tree.leaf_node_num_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6bd789b-733b-41fc-a0ab-32558211f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tree.leaf_nodes(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd900c0-23c6-42ba-a8d5-4d501c00a54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4929],\n",
       "        [1.4926],\n",
       "        [1.4917],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4930],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4907],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4929],\n",
       "        [1.4930],\n",
       "        [1.4932],\n",
       "        [1.4924],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4929],\n",
       "        [1.4932],\n",
       "        [1.4921],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4931],\n",
       "        [1.4922],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4895],\n",
       "        [1.4932],\n",
       "        [1.4874],\n",
       "        [1.4932],\n",
       "        [1.4928],\n",
       "        [1.4930],\n",
       "        [1.4929],\n",
       "        [1.4922],\n",
       "        [1.4922],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4931],\n",
       "        [1.4909],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4921],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4929],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4931],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4928],\n",
       "        [1.4930],\n",
       "        [1.4929],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4931],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4928],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4930],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4931],\n",
       "        [1.4931],\n",
       "        [1.4931],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4930],\n",
       "        [1.4932],\n",
       "        [1.4926],\n",
       "        [1.4931],\n",
       "        [1.4931],\n",
       "        [1.4931],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4930],\n",
       "        [1.4932],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4922],\n",
       "        [1.4925],\n",
       "        [1.4932],\n",
       "        [1.4928],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4919],\n",
       "        [1.4931],\n",
       "        [1.4931],\n",
       "        [1.4906],\n",
       "        [1.4917],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4930],\n",
       "        [1.4929],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4897],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4924],\n",
       "        [1.4921],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4930],\n",
       "        [1.4932],\n",
       "        [1.4931],\n",
       "        [1.4929],\n",
       "        [1.4932],\n",
       "        [1.4931],\n",
       "        [1.4932],\n",
       "        [1.4932],\n",
       "        [1.4932]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
